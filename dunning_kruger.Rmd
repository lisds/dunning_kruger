---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Exploring the Dunning-Kruger effect

Here we load the libraries we need:

```{python}
import numpy as np
rng = np.random.default_rng()
import pandas as pd
import matplotlib.pyplot as plt
np.set_printoptions(suppress=True)
pd.set_option('mode.copy_on_write', True)
```

In this notebook we are investigating whether the Dunning-Kruger effect can be explained with simple statistical artifacts.

An example of random choice.

```{python}
a = rng.choice([1, 2], p=[0.3, 0.7], size=10)
a
```

Make some actual scores.

```{python}
# Actual scores from some test.  Mean 50, std 15.
actual_scores = rng.normal(50, 15, size=10_000)
actual_scores[:10]
```

```{python}
plt.hist(actual_scores, bins=100);
```

```{python}
# Actually, I'm more interested in percentiles.  Let's order the scores.
sorted_actual = np.sort(actual_scores)
sorted_actual
```

```{python}
# Oh wait, it doesn't actually matter what the scores are, it just depends
# on their rank order.   I can just use linspace for that.
percentile_actual = np.linspace(0, 100, 10_000)
percentile_actual
```

```{python}
#People are unlikely to rank themselves in the bottom/ top 20% so each end only contains 5%
#of the data. The Bta effect is accounted for, 55% of the values fall between 50-80 only 35% 
#between 20 and 50. I think this is better than the crushed model as i don't think it's justifiable 
#to assume that no one will rate themselves in the top or bottom 20%, people can be seriously cocky
#and self depricating. 
zero_to_10 = np.linspace(0, 20, 500)
ten_to_50 = np.linspace(20, 50, 3500)
fifty_to_90 = np.linspace(50, 80, 5500)
ninety_to_100 = np.linspace(80, 100, 500)
first_half = np.append(zero_to_10, ten_to_50)
second_half = np.append(fifty_to_90, ninety_to_100)
full = np.append(first_half, second_half)

```

```{python}


```

```{python}
plt.hist(full, bins=100)
```

```{python}
perceived_minus_actual = full - percentile_actual
perceived_minus_actual
```

```{python}
plt.hist(perceived_minus_actual)
```

```{python}
plt.plot(perceived_minus_actual)
```

```{python}
df = pd.DataFrame({
    'actual_percentile': percentile_actual,
    'perceived_percentile': full,
    'difference': full - percentile_actual
})
df
```

```{python}
df['actual_quantile'] = pd.qcut(df['actual_percentile'], 4,
                                labels=['low', 'med-low', 'med-high', 'high'])
df
```

```{python}
df.groupby('actual_quantile', observed=True)['difference'].mean()
```

```{python}
# add some randomness to perceived percentiles
df['perceived_percentile_w_randomness'] = df['perceived_percentile'] + rng.normal(0, 10, len(df))

# calculate the difference between the new perceived percentiles (with randomness) and the actual percentiles
df['difference_w_randomness'] = df['perceived_percentile_w_randomness'] - df['actual_percentile']

df.head()
```

```{python}

```

```{python}

```
