---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Exploring the Dunning-Kruger effect

Here we load the libraries we need:

```{python}
import numpy as np
rng = np.random.default_rng()
import pandas as pd
import matplotlib.pyplot as plt
np.set_printoptions(suppress=True)
pd.set_option('mode.copy_on_write', True)
```

In this notebook we are investigating whether the Dunning-Kruger effect can be explained with simple statistical artifacts.

An example of random choice.

```{python}
a = rng.choice([1, 2], p=[0.3, 0.7], size=10)
a
```

Make some actual scores.

```{python}
# Actual scores from some test.  Mean 50, std 15.
actual_scores = rng.normal(50, 15, size=10_000)
actual_scores[:10]
```

```{python}
plt.hist(actual_scores, bins=100);
```

```{python}
# Actually, I'm more interested in percentiles.  Let's order the scores.
sorted_actual = np.sort(actual_scores)
sorted_actual
```

```{python}
# Oh wait, it doesn't actually matter what the scores are, it just depends
# on their rank order.   I can just use linspace for that.
percentile_actual = np.linspace(0, 100, 10_000)
percentile_actual
```

```{python}
# Scale the range either side of the mean, to +-30 (rather than 50)
x = percentile_actual -  50. # Scale so range is -30 to +30.
crushed_bta = x * (30 / 50) + 60. # BTA effect (60 not 50).
np.mean(crushed_bta)
```

```{python}
plt.hist(crushed_bta, bins=100)
```

```{python}
perceived_minus_actual = crushed_bta - percentile_actual
perceived_minus_actual
```

```{python}
plt.hist(perceived_minus_actual)
```

```{python}
plt.plot(perceived_minus_actual)
```

```{python}
df = pd.DataFrame({
    'actual_percentile': percentile_actual,
    'perceived_percentile': crushed_bta,
    'difference': crushed_bta - percentile_actual
})
df
```

```{python}
df['actual_quantile'] = pd.qcut(df['actual_percentile'], 4,
                                labels=['low', 'med-low', 'med-high', 'high'])
df
```

```{python}
df.groupby('actual_quantile', observed=True)['difference'].mean()
```

```{python}
# add some randomness to perceived percentiles
df['perceived_percentile_w_randomness'] = df['perceived_percentile'] + rng.normal(0, 10, len(df))

# calculate the difference between the new perceived percentiles (with randomness) and the actual percentiles
df['difference_w_randomness'] = df['perceived_percentile_w_randomness'] - df['actual_percentile']

df.head(101)
```

### Initial Version

```{python}
#creating a new simulation of perceived scores which takes into account the better than average effect,
#and realistically distributes people at the top and bottom of the distribution
perceived_scores = rng.normal(60, 15, size=10_000)
plt.hist(perceived_scores, bins = 100);
```

### Improved Model

```{python}
#I like the lower half of the distribution above, however, there are people distributed significantly beyond 100%
#I believe that people above the median would be slightly more concentrated towards the median than those before it
#This is because people towards the higher percentiles may want to appear as more modest
lower_half = np.sort(perceived_scores)[:5000]
upper_half = np.sort(rng.normal(60, 12, size = 10_000))[5000:]
perceived_dist = np.concatenate((lower_half,upper_half))
plt.hist(perceived_dist,bins=105);
plt.xlim([0,105]);

```

```{python}
perceived_percentiles = np.percentile(perceived_dist, np.arange(0,101,1))
actual_percentiles = np.percentile(actual_scores, np.arange(0,101,1))
df2 = pd.DataFrame({
    'actual_percentile': actual_percentiles,
    'perceived_percentile': perceived_percentiles})
df2
```

```{python}
df2['actual - perceived'] = df2['actual_percentile'] - df2['perceived_percentile']
lower_quartile_mean = np.mean(df2['actual - perceived'][:25])
upper_quartile_mean = np.mean(df2['actual - perceived'][75:])
print("The mean difference between actual and perceived, across percentiles in the lower quartile is", str(lower_quartile_mean)[:5])
print("In the upper quartile it is", str(upper_quartile_mean)[:4])
```

This means that for the lower quartile, people perceive themselves as being more intelligent then they are. 
In the upper quartile, people perceive themselves as being less intelligent then they are.  
Both of these align with the hypothesis of the Dunning-Kruger effect. Therefore I have, at least based on these metrics, replicated the Dunning-Kruger effect.  
I prefer my model to the crushed score model because I think it is unrealistic for there to be no individuals in a sample of 10,000 who would place themselves within the bottom 30% for intelligence. Furthermore, a large number of datapoints in the crushed score model are significantly beyond the 100% score, while for my model only a small number of datapoints enter this impossible area.
