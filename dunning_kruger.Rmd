---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Exploring the Dunning-Kruger effect

Here we load the libraries we need:

```{python}
import numpy as np
rng = np.random.default_rng()
import pandas as pd
import matplotlib.pyplot as plt
np.set_printoptions(suppress=True)
pd.set_option('mode.copy_on_write', True)
```

In this notebook we are investigating whether the Dunning-Kruger effect can be explained with simple statistical artifacts.

An example of random choice.

```{python}
a = rng.choice([1, 2], p=[0.3, 0.7], size=10)
a
```

Make some actual scores.

```{python}
# Actual scores from some test.  Mean 50, std 15.
actual_scores = rng.normal(50, 15, size=10_000)
actual_scores[:10]
```

```{python}
plt.hist(actual_scores, bins=100);
```

```{python}
# Actually, I'm more interested in percentiles.  Let's order the scores.
sorted_actual = np.sort(actual_scores)
sorted_actual
```

```{python}
# Oh wait, it doesn't actually matter what the scores are, it just depends
# on their rank order.   I can just use linspace for that.
percentile_actual = np.linspace(0, 100, 10_000)
percentile_actual
```

```{python}
# Scale the range either side of the mean, to +-30 (rather than 50)
x = percentile_actual -  50. # Scale so range is -30 to +30.
crushed_bta = x * (30 / 50) + 60. # BTA effect (60 not 50).
np.mean(crushed_bta)
```

```{python}
plt.hist(crushed_bta, bins=100)
```

```{python}
perceived_minus_actual = crushed_bta - percentile_actual
perceived_minus_actual
```

```{python}
plt.hist(perceived_minus_actual)
```

```{python}
plt.plot(perceived_minus_actual)
```

```{python}
df = pd.DataFrame({
    'actual_percentile': percentile_actual,
    'perceived_percentile': crushed_bta,
    'difference': crushed_bta - percentile_actual
})
df
```

```{python}
df['actual_quantile'] = pd.qcut(df['actual_percentile'], 4,
                                labels=['low', 'med-low', 'med-high', 'high'])
df
```

```{python}
df.groupby('actual_quantile', observed=True)['difference'].mean()
```

### My edits bellow

For this simulation I'm going to assume that people have no idea how good they are and so there percieved score will br random. However I will also assume that no one will vote themselves less than the tenth percentile and greter than the 90th percentile, because of this the distribution will also be a little smaller.  The mean for the percieved score will alos be higher than the average hifglighting the better than average.  

```{python}
perceived_percentile_2 = rng.normal(60, 10, size=10_000)

perceived_percentile_2 = np.clip(perceived_percentile_2, 10, 90)

plt.hist(percieved_percentile_2, bins=100);
```

```{python}
df_2 = pd.DataFrame({
    'actual_percentile': percentile_actual,
    'perceived_percentile': perceived_percentile_2,
    'difference': perceived_percentile_2 - percentile_actual
})
df_2
```

```{python}
df_2['actual_quantile'] = pd.qcut(df['actual_percentile'], 4,
                                labels=['low', 'med-low', 'med-high', 'high'])
df_2
```

```{python}
df_2.groupby('actual_quantile', observed=True)['difference'].mean()
```

According to the results I've got here my assumptions could explain the Dunning-Kruger effect, however i will do this simulate this a few more times to esnure this is true. 

```{python}
num_runs = 100

all_results = []

for i in range(num_runs):
   
    perceived_percentile_2 = np.random.normal(60, 10, size=10_000)

    perceived_percentile_2 = np.clip(perceived_percentile_2, 10, 90)

    df_2 = pd.DataFrame({
        'actual_percentile': percentile_actual,
        'perceived_percentile': perceived_percentile_2,
        'difference': perceived_percentile_2 - percentile_actual
    })

    df_2['actual_quantile'] = pd.qcut(df_2['actual_percentile'], 4, labels=['low', 'med-low', 'med-high', 'high'])

    mean_difference_by_quantile = df_2.groupby('actual_quantile', observed=True)['difference'].mean()

    all_results.append(mean_difference_by_quantile)

all_results_df = pd.concat(all_results, axis=1)

mean_diff_across_runs = all_results_df.mean(axis=1)
std_diff_across_runs = all_results_df.std(axis=1)


print("Mean difference across runs:\n", mean_diff_across_runs)
print("\nStandard deviation of difference across runs:\n", std_diff_across_runs)

```

This aslo indicates the same results as the Dunning Kruger effect.
